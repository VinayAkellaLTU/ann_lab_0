{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1 \n",
    "1. Load CIFAR1-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load CIFAR-10 Dataset\n",
    "transform = transforms.ToTensor()\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device I have is cuda\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, activation_fn=nn.LeakyReLU()):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # The first layer has 3 input channels and 32 output channels. The point here is\n",
    "        # that the CIFR images are 32 * 32, and RGB channels after the to tensor. So, we take\n",
    "        self.conv_layer = nn.Conv2d(3, 32, kernel_size=5) \n",
    "        # Fully connected layer - the hidden layers are 28 * 28 hence and pooling reduces it by 2\n",
    "        # Hence, 14 * 14 and mult that with 32 feature/conv layers.\n",
    "        self.nn_layer = nn.Linear(32 * 14 * 14, 10)\n",
    "        self.activation = activation_fn  # Activation function\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv_layer(x))) \n",
    "        x = torch.flatten(x, 1)  # Flatten the feature maps, as the next layer is 1d layer\n",
    "        x = self.nn_layer(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# ANN-1 excercise\n",
    "def train_model(model, optimizer, error_func, device, epochs=10, log_dir=\"\"):\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in trainloader:\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = error_func(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = 100 * correct / total  # Compute accuracy\n",
    "        writer.add_scalar('Loss/train', running_loss / len(trainloader), epoch)  # Log loss\n",
    "        writer.add_scalar('Accuracy/train', train_acc, epoch)  # Log accuracy\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}, Accuracy: {train_acc}%')\n",
    "    writer.close()\n",
    "\n",
    "# Train model with LeakyReLU and SGD optimizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The device I have is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3018196123030483, Accuracy: 12.056%\n",
      "Epoch 2, Loss: 2.283367352717368, Accuracy: 14.964%\n",
      "Epoch 3, Loss: 2.2679799975031782, Accuracy: 18.66%\n",
      "Epoch 4, Loss: 2.2523631528210455, Accuracy: 21.394%\n",
      "Epoch 5, Loss: 2.2362577613357386, Accuracy: 22.874%\n",
      "Epoch 6, Loss: 2.2195140336785477, Accuracy: 24.066%\n",
      "Epoch 7, Loss: 2.202300337574366, Accuracy: 24.944%\n",
      "Epoch 8, Loss: 2.1845530448362345, Accuracy: 25.92%\n",
      "Epoch 9, Loss: 2.167002440718434, Accuracy: 26.12%\n",
      "Epoch 10, Loss: 2.14963867017985, Accuracy: 26.972%\n",
      "Epoch 1, Loss: 1.507934270612419, Accuracy: 47.094%\n",
      "Epoch 2, Loss: 1.230274350518156, Accuracy: 57.606%\n",
      "Epoch 3, Loss: 1.124247382097232, Accuracy: 61.48%\n",
      "Epoch 4, Loss: 1.0578128779330827, Accuracy: 63.616%\n",
      "Epoch 5, Loss: 1.0119071758311728, Accuracy: 65.146%\n",
      "Epoch 6, Loss: 0.9736140230884942, Accuracy: 66.57%\n",
      "Epoch 7, Loss: 0.9415218439096075, Accuracy: 67.808%\n",
      "Epoch 8, Loss: 0.9150028026393612, Accuracy: 68.588%\n",
      "Epoch 9, Loss: 0.8918096398758462, Accuracy: 69.412%\n",
      "Epoch 10, Loss: 0.8742049944294078, Accuracy: 70.01%\n",
      "Epoch 1, Loss: 1.6322678333658087, Accuracy: 42.974%\n",
      "Epoch 2, Loss: 1.4112999334054834, Accuracy: 51.288%\n",
      "Epoch 3, Loss: 1.2800296673079585, Accuracy: 55.994%\n",
      "Epoch 4, Loss: 1.1980498080973125, Accuracy: 59.016%\n",
      "Epoch 5, Loss: 1.145987006709399, Accuracy: 60.828%\n",
      "Epoch 6, Loss: 1.1024850693810018, Accuracy: 62.414%\n",
      "Epoch 7, Loss: 1.074030508546878, Accuracy: 63.3%\n",
      "Epoch 8, Loss: 1.0490639433836388, Accuracy: 64.12%\n",
      "Epoch 9, Loss: 1.0199592326913038, Accuracy: 65.106%\n",
      "Epoch 10, Loss: 1.0007350060641003, Accuracy: 65.666%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model1 = SimpleCNN(nn.LeakyReLU())\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.0001)\n",
    "err_func = nn.CrossEntropyLoss()\n",
    "train_model(model1, optimizer1, err_func,device, 10, log_dir='logs/leakyrelu_sgd')\n",
    "\n",
    "# Train model with LeakyReLU and Adam optimizer\n",
    "model2 = SimpleCNN(nn.LeakyReLU())\n",
    "optimizer2 = optim.Adam(model2.parameters())\n",
    "train_model(model2, optimizer2, err_func,device, 10, log_dir='logs/leakyrelu_adam')\n",
    "\n",
    "# Train model with Tanh and Adam optimizer\n",
    "model3 = SimpleCNN(nn.Tanh())\n",
    "optimizer3 = optim.Adam(model3.parameters())\n",
    "train_model(model3, optimizer3, err_func,device, 10, log_dir='logs/tanh_adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.749811439715383, Accuracy: 34.37%\n",
      "Epoch 2, Loss: 1.3875869479020844, Accuracy: 49.598%\n",
      "Epoch 3, Loss: 1.2480666821112718, Accuracy: 55.462%\n",
      "Epoch 4, Loss: 1.1492799949615509, Accuracy: 59.346%\n",
      "Epoch 5, Loss: 1.0784562936676738, Accuracy: 61.984%\n",
      "Epoch 6, Loss: 1.0219819440561182, Accuracy: 64.004%\n",
      "Epoch 7, Loss: 0.9642485929724506, Accuracy: 65.928%\n",
      "Epoch 8, Loss: 0.9198754125696313, Accuracy: 67.728%\n",
      "Epoch 9, Loss: 0.8928725300999858, Accuracy: 68.604%\n",
      "Epoch 10, Loss: 0.8593373371435858, Accuracy: 69.632%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LTU\\AdvancedNN\\ann_lab_0\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8791558714321507, Accuracy: 69.098%\n",
      "Epoch 2, Loss: 0.779891442612309, Accuracy: 72.726%\n",
      "Epoch 3, Loss: 0.7589350004330315, Accuracy: 73.352%\n",
      "Epoch 4, Loss: 0.7487478452494077, Accuracy: 73.59%\n",
      "Epoch 5, Loss: 0.7383266522375214, Accuracy: 74.126%\n"
     ]
    }
   ],
   "source": [
    "# 2. Fine-tuning the AlexNet model for CIFAR-10\n",
    "transform_alex = transforms.Compose([\n",
    "    transforms.Resize(224),  # Resize to 224x224 to match AlexNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_alex)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_alex)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "alexnet = models.alexnet(pretrained=False)  # We will initialize AlexNet without pre-trained weights\n",
    "alexnet.classifier[6] = nn.Linear(in_features=4096, out_features=10) \n",
    "optimizer_alexnet = optim.Adam(alexnet.parameters(), lr=0.001)\n",
    "err_func = nn.CrossEntropyLoss()\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "train_model(alexnet, optimizer_alexnet, err_func,device, 10, log_dir='logs/alexnet_finetuning')\n",
    "\n",
    "alexnet_pretrained = models.alexnet(pretrained=True)\n",
    "\n",
    "\n",
    "alexnet_pretrained.classifier[6] = nn.Linear(in_features=4096, out_features=10)\n",
    "optimizer_pretrained = optim.Adam(alexnet_pretrained.classifier[6].parameters())\n",
    "\n",
    "train_model(alexnet_pretrained, optimizer_pretrained, err_func,device, 5, log_dir='logs/alexnet_feature_extraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.22566888012540048, Accuracy: 93.35%\n",
      "Epoch 2, Loss: 0.08500903119136498, Accuracy: 97.435%\n",
      "Epoch 3, Loss: 0.06205552467815221, Accuracy: 98.08166666666666%\n",
      "Epoch 4, Loss: 0.052215030739656776, Accuracy: 98.34%\n",
      "Epoch 5, Loss: 0.04461047726684311, Accuracy: 98.60666666666667%\n"
     ]
    }
   ],
   "source": [
    "#Using the same CNN for MINST and SVHN datsets.\n",
    "\n",
    "\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to match your model's expected input size\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert 1 channel -> 3 channels\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
    "\n",
    "trainloader = DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(mnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "model_minst = SimpleCNN(nn.Tanh())\n",
    "optimizer_minst = optim.Adam(model_minst.parameters())\n",
    "\n",
    "# optimizer1 = optim.SGD(model1.parameters(), lr=0.0001)\n",
    "err_func = nn.CrossEntropyLoss()\n",
    "train_model(model_minst, optimizer_minst, err_func,device, 5, log_dir='logs/minst_tanh_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2139931849803704, Accuracy: 93.75833333333334%\n",
      "Epoch 2, Loss: 0.08197729043942541, Accuracy: 97.51833333333333%\n",
      "Epoch 3, Loss: 0.0652031527945339, Accuracy: 97.96%\n",
      "Epoch 4, Loss: 0.05285450167157677, Accuracy: 98.40333333333334%\n",
      "Epoch 5, Loss: 0.04694962119831314, Accuracy: 98.5%\n"
     ]
    }
   ],
   "source": [
    "transform_svhn = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "svhn_trainset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=transform_svhn)\n",
    "svhn_testset = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=transform_svhn)\n",
    "\n",
    "svhn_trainloader = DataLoader(svhn_trainset, batch_size=64, shuffle=True)\n",
    "svhn_testloader = DataLoader(svhn_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "model_svhn = SimpleCNN(nn.Tanh())\n",
    "optimizer_svhn = optim.Adam(model_svhn.parameters())\n",
    "\n",
    "# optimizer1 = optim.SGD(model1.parameters(), lr=0.0001)\n",
    "err_func = nn.CrossEntropyLoss()\n",
    "train_model(model_svhn, optimizer_svhn, err_func,device, 5, log_dir='logs/svhn_tanh_adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
